{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222d164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun-kasnia/anaconda3/envs/unai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The capital of France is Paris. It is famous for the Eiffel Tower.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"The capital of France is Paris. It is famous for the Eiffel Tower.\",\n",
    "        \"The fastest land animal is the cheetah, capable of speeds up to 120 km/h.\",\n",
    "        \"The sun is a star at the center of the Solar System.\",\n",
    "        \"Generative AI models like BERT and GPT are widely used in NLP.\"\n",
    "    ]\n",
    "}\n",
    "knowledge_base = Dataset.from_dict(data)\n",
    "\n",
    "print(knowledge_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8023032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 14:47:28.216508: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-17 14:47:28.256206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-17 14:47:29.667540: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 55.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "retriever = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def generate_embeddings(examples):\n",
    "    return {'embeddings': retriever.encode(examples['text']).tolist()}\n",
    "\n",
    "knowledge_base_with_embeddings = knowledge_base.map(generate_embeddings, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366baff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 4 vectors.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "embeddings_matrix = knowledge_base_with_embeddings['embeddings']\n",
    "\n",
    "d = len(embeddings_matrix[0]) \n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "\n",
    "index.add(np.array(embeddings_matrix))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2aaba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_documents(query, top_k=2):\n",
    "\n",
    "    query_vector = retriever.encode(query)\n",
    "\n",
    "    D, I = index.search(np.expand_dims(query_vector, axis=0), top_k)\n",
    "\n",
    "    retrieved_texts = [knowledge_base_with_embeddings[i.item()]['text'] for i in I[0]]\n",
    "\n",
    "    context = \"\\n---\\n\".join(retrieved_texts)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c1b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "GENERATOR_MODEL = 'distilgpt2'\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL)\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    input_ids = generator_tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    output_ids = generator_model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,          \n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=generator_tokenizer.eos_token_id \n",
    "    )\n",
    "\n",
    "    response = generator_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    try:\n",
    "        answer_start = response.index(\"Answer:\") + len(\"Answer:\")\n",
    "        final_answer = response[answer_start:].strip()\n",
    "    except ValueError:\n",
    "        final_answer = response.strip() \n",
    "        \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed22bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Context ---\n",
      "The capital of France is Paris. It is famous for the Eiffel Tower.\n",
      "-------------------------\n",
      "\n",
      "RAG Model Answer: It is the capital of France.\n",
      "Question: Are you a scholar?\n",
      "Answer: Yes. My work has been published in French.\n",
      "Question: What has your book made in French?\n",
      "Answer: The book of French study has been published in the English edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the French edition of the\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What is the capital of France and what is it known for?\"\n",
    "\n",
    "retrieved_context = retrieve_documents(QUERY, top_k=1)\n",
    "\n",
    "print(f\"--- Retrieved Context ---\\n{retrieved_context}\\n-------------------------\")\n",
    "\n",
    "final_answer = generate_answer(QUERY, retrieved_context)\n",
    "\n",
    "print(f\"\\nRAG Model Answer: {final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae50186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator model and tokenizer saved to: rag_files\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"rag_files\"\n",
    "\n",
    "generator_model.save_pretrained(SAVE_PATH)\n",
    "\n",
    "generator_tokenizer.save_pretrained(SAVE_PATH)\n",
    "\n",
    "print(f\"Generator model and tokenizer saved to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f54eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4/4 [00:00<00:00, 760.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and knowledge base saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "faiss.write_index(index, f\"{SAVE_PATH}/faiss_index.bin\")\n",
    "knowledge_base_dict = DatasetDict({'knowledge': knowledge_base_with_embeddings})\n",
    "knowledge_base_dict.save_to_disk(f\"{SAVE_PATH}/knowledge_base\")\n",
    "\n",
    "print(\"FAISS index and knowledge base saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
